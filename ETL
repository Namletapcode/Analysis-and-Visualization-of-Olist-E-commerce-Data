{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/duysakura/visualization-2-0?scriptVersionId=294140599\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:24:59.889197Z","iopub.execute_input":"2026-01-25T03:24:59.890385Z","iopub.status.idle":"2026-01-25T03:25:05.141809Z","shell.execute_reply.started":"2026-01-25T03:24:59.890347Z","shell.execute_reply":"2026-01-25T03:25:05.140688Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import boto3\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom kaggle_secrets import UserSecretsClient","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:25:05.144486Z","iopub.execute_input":"2026-01-25T03:25:05.144854Z","iopub.status.idle":"2026-01-25T03:25:06.405133Z","shell.execute_reply.started":"2026-01-25T03:25:05.144788Z","shell.execute_reply":"2026-01-25T03:25:06.40408Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nMINIO_ACCESS_KEY = user_secrets.get_secret(\"MINIO_ACCESS_KEY\")\nMINIO_ENDPOINT = user_secrets.get_secret(\"MINIO_ENDPOINT\")\nMINIO_SECRET_KEY = user_secrets.get_secret(\"MINIO_SECRET_KEY\")\nMINIO_BUCKET_NAME = 'olist-data'\nMINIO_CONFIG = {\n    'endpoint_url': MINIO_ENDPOINT,\n    'aws_access_key_id': MINIO_ACCESS_KEY,\n    'aws_secret_access_key': MINIO_SECRET_KEY\n}\n\nMYSQL_URL = user_secrets.get_secret(\"MYSQL_URL\")\nMYSQL_PROPERTIES = {\n    'user': user_secrets.get_secret(\"MYSQL_USER\"),\n    'password': user_secrets.get_secret(\"MYSQL_PASSWORD\"),\n    'driver': user_secrets.get_secret(\"MYSQL_DRIVER\")\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:29:02.150898Z","iopub.execute_input":"2026-01-25T03:29:02.151925Z","iopub.status.idle":"2026-01-25T03:29:03.707865Z","shell.execute_reply.started":"2026-01-25T03:29:02.151886Z","shell.execute_reply":"2026-01-25T03:29:03.70683Z"}},"outputs":[{"name":"stdout","text":" https://josue-unsonant-desire.ngrok-free.dev\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def write_to_mysql(df, table_name):\n    try:\n        jdbc_url = MYSQL_URL\n\n        params = [\n            \"rewriteBatchedStatements=true\",\n            \"allowPublicKeyRetrieval=true\",\n            \"useSSL=false\"\n        ]\n\n        if \"?\" in jdbc_url:\n            jdbc_url = jdbc_url + \"&\" + \"&\".join(params)\n        else:\n            jdbc_url = jdbc_url + \"?\" + \"&\".join(params)\n\n        df.coalesce(4).write.jdbc(\n            url=jdbc_url,\n            table=table_name,\n            mode='overwrite',\n            properties={\n                **MYSQL_PROPERTIES,\n                \"batchsize\": \"5000\",\n                \"isolationLevel\": \"NONE\"\n            }\n        )\n        \n        print(f\"Lưu bảng {table_name} thành công\")\n        \n    except Exception as e:\n        print(f\"Lỗi khi ghi bảng {table_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:25:08.045612Z","iopub.execute_input":"2026-01-25T03:25:08.046194Z","iopub.status.idle":"2026-01-25T03:25:08.052788Z","shell.execute_reply.started":"2026-01-25T03:25:08.046152Z","shell.execute_reply":"2026-01-25T03:25:08.051885Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"packages = [\n    \"org.apache.hadoop:hadoop-aws:3.3.4\",            \n    \"com.amazonaws:aws-java-sdk-bundle:1.12.540\",\n    \"com.mysql:mysql-connector-j:8.2.0\"\n]\n\nspark = SparkSession.builder \\\n    .appName(\"Olist's Brazilian E-Commerce\") \\\n    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n\nspark.sparkContext.setLogLevel(\"WARN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:25:08.054118Z","iopub.execute_input":"2026-01-25T03:25:08.054515Z","iopub.status.idle":"2026-01-25T03:25:26.873682Z","shell.execute_reply.started":"2026-01-25T03:25:08.054475Z","shell.execute_reply":"2026-01-25T03:25:26.872467Z"}},"outputs":[{"name":"stdout","text":":: loading settings :: url = jar:file:/usr/local/lib/python3.12/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n","output_type":"stream"},{"name":"stderr","text":"Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\norg.apache.hadoop#hadoop-aws added as a dependency\ncom.amazonaws#aws-java-sdk-bundle added as a dependency\ncom.mysql#mysql-connector-j added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-82776874-343c-4af6-a6ab-f38462a5312e;1.0\n\tconfs: [default]\n\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n\tfound com.amazonaws#aws-java-sdk-bundle;1.12.540 in central\n\tfound com.mysql#mysql-connector-j;8.2.0 in central\n\tfound com.google.protobuf#protobuf-java;3.21.9 in central\ndownloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (104ms)\ndownloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.540/aws-java-sdk-bundle-1.12.540.jar ...\n\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.540!aws-java-sdk-bundle.jar (1731ms)\ndownloading https://repo1.maven.org/maven2/com/mysql/mysql-connector-j/8.2.0/mysql-connector-j-8.2.0.jar ...\n\t[SUCCESSFUL ] com.mysql#mysql-connector-j;8.2.0!mysql-connector-j.jar (62ms)\ndownloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (48ms)\ndownloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.21.9/protobuf-java-3.21.9.jar ...\n\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.21.9!protobuf-java.jar(bundle) (56ms)\n:: resolution report :: resolve 5991ms :: artifacts dl 2013ms\n\t:: modules in use:\n\tcom.amazonaws#aws-java-sdk-bundle;1.12.540 from central in [default]\n\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n\tcom.mysql#mysql-connector-j;8.2.0 from central in [default]\n\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n\t:: evicted modules:\n\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.540] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   6   |   5   |   5   |   1   ||   5   |   5   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-82776874-343c-4af6-a6ab-f38462a5312e\n\tconfs: [default]\n\t5 artifacts copied, 0 already retrieved (337425kB/348ms)\n26/01/25 03:25:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"s3 = boto3.client('s3', **MINIO_CONFIG)\n\ntry:\n    response = s3.list_objects_v2(Bucket=MINIO_BUCKET_NAME)\n\n    if 'Contents' not in response:\n        print(\"Bucket rỗng\")\n        sys.exit(0)\n\n    # I. Load dữ liệu thô từ MinIO và chuyển thành bảng\n    base_url = f\"s3a://{MINIO_BUCKET_NAME}\"\n    for obj in response['Contents']:\n        file_name = obj['Key']\n\n        if not file_name.endswith('.csv'):\n            continue\n\n        df = spark.read.csv(f\"{base_url}/{file_name}\", header=True, multiLine=True, inferSchema=True)\n        table_name = file_name.replace('olist_', '').replace('_dataset.csv', '').replace('.csv', '')\n        df.createOrReplaceTempView(table_name)\n\n    # II. Làm sạch dữ liệu\n    # 1. Chuyển tên sản phẩm trong products về tiếng Anh\n    df_clean_products = spark.sql(\"\"\"\n        select \n            p.*,\n            pt.product_category_name_english\n        from products p\n        left join product_category_name_translation pt on p.product_category_name = pt.product_category_name\n    \"\"\")\n    df_clean_products = df_clean_products.drop(\"product_category_name\")\n    df_clean_products.createOrReplaceTempView(\"products\")\n\n    # 2. Chuẩn hóa geolocation với tọa độ là trung bình các tọa độ và tên thành phố theo chuẩn Title Case, không dấu\n    df_clean_geolocation = spark.sql(\"\"\"\n        select \n            geolocation_zip_code_prefix,\n            avg(geolocation_lat) geolocation_lat,\n            avg(geolocation_lng) geolocation_lng,\n            initcap(\n                translate(\n                    first(geolocation_city), \n                    'áàảãạâấầẩẫậăắằẳẵặéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỊĐ',\n                    'aaaaaaaaaaaaaaaaaeeeeeeeeeeeiiiiioooooooooooooooooouuuuuuuuuuuyyyyydAAAAAAAAAAAAAAAAAEEEEEEEEEEEIIIIIOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYD'\n                )\n            ) geolocation_city,\n            first(geolocation_state) geolocation_state\n        from geolocation\n        group by geolocation_zip_code_prefix\n    \"\"\")\n    df_clean_geolocation.createOrReplaceTempView(\"geolocation\")\n\n    # 3. Loại bỏ các đơn hàng không giao thành công\n    df_clean_orders = spark.sql(\"\"\"\n        select * from orders\n        where order_delivered_customer_date is not null\n    \"\"\")\n    df_clean_orders.createOrReplaceTempView(\"orders\")\n\n    # 4. Loại bỏ những reviews có ngày tháng không hợp lệ và chuẩn hóa kiểu dữ liệu của các cột ngày tháng\n    df_clean_order_reviews = spark.sql(\"\"\"\n        select * from order_reviews\n        where review_creation_date is not null and review_answer_timestamp is not null\n    \"\"\")\n    df_clean_order_reviews = df_clean_order_reviews \\\n        .withColumn(\"review_creation_date\", col(\"review_creation_date\").cast(\"timestamp\")) \\\n        .withColumn(\"review_answer_timestamp\", col(\"review_answer_timestamp\").cast(\"timestamp\")) \\\n        .withColumnRenamed(\"review_answer_timestamp\", \"review_answer_date\")\n    df_clean_order_reviews.createOrReplaceTempView(\"order_reviews\")\n\n    # III. Làm giàu dữ liệu\n    # 1. Tính thời gian giao hàng và kiểm tra đơn hàng có bị giao muộn không\n    df_enrich_orders = spark.sql(\"\"\"\n        select \n            *,\n            datediff(order_delivered_customer_date, order_purchase_timestamp) actual_delivery_days,\n            (order_delivered_customer_date > order_estimated_delivery_date) is_late\n        from orders\n    \"\"\")\n    df_enrich_orders.createOrReplaceTempView(\"orders\")\n\n    # 2. Tính tổng giá trị đơn hàng trong order_items\n    df_enrich_order_items = spark.sql(\"\"\"\n        select \n            *,\n            round((price + freight_value), 2) total_item_value\n        from order_items\n    \"\"\")\n    df_enrich_order_items.createOrReplaceTempView(\"order_items\")\n\n    # 3. Phân loại các thành phố, bang theo khu vực\n    region_data = [\n        ('AC', 'North'), ('AP', 'North'), ('AM', 'North'), ('PA', 'North'), ('RO', 'North'), ('RR', 'North'), ('TO', 'North'),\n        ('AL', 'Northeast'), ('BA', 'Northeast'), ('CE', 'Northeast'), ('MA', 'Northeast'), ('PB', 'Northeast'), ('PE', 'Northeast'), ('PI', 'Northeast'), ('RN', 'Northeast'), ('SE', 'Northeast'),\n        ('DF', 'Central-West'), ('GO', 'Central-West'), ('MT', 'Central-West'), ('MS', 'Central-West'),\n        ('ES', 'Southeast'), ('MG', 'Southeast'), ('RJ', 'Southeast'), ('SP', 'Southeast'),\n        ('PR', 'South'), ('RS', 'South'), ('SC', 'South')\n    ]\n    df_regions = spark.createDataFrame(region_data, [\"geolocation_state\", \"region\"])\n    df_regions.createOrReplaceTempView(\"regions\")\n    df_enrich_geolocation = spark.sql(\"\"\"\n        select\n            g.*,\n            r.region geolocation_region\n        from geolocation g\n        left join regions r on g.geolocation_state = r.geolocation_state\n    \"\"\")\n    df_enrich_geolocation.createOrReplaceTempView(\"geolocation\")\n\n    # IV. Mô hình hóa\n    df_fact_order_items = spark.sql(\"\"\"\n        select \n            o.order_id,\n            o.customer_id,\n            oi.product_id,\n            oi.seller_id,\n            o.order_purchase_timestamp,\n            o.order_delivered_customer_date,\n            oi.price,\n            oi.freight_value,\n            oi.total_item_value,\n            o.actual_delivery_days,\n            o.is_late\n        from orders o\n        join order_items oi on o.order_id = oi.order_id\n    \"\"\")\n\n    df_dim_customers = spark.sql(\"\"\"\n        select \n            c.customer_id,\n            c.customer_unique_id,\n            g.*\n        from customers c\n        left join geolocation g on c.customer_zip_code_prefix = g.geolocation_zip_code_prefix\n    \"\"\")\n\n    df_dim_products = spark.sql(\"\"\"\n        select \n            product_id,\n            product_category_name_english product_category_name,\n            product_photos_qty,\n            product_weight_g,\n            product_length_cm,\n            product_height_cm,\n            product_width_cm\n        from products\n    \"\"\")\n\n    df_dim_sellers = spark.sql(\"\"\"\n        select \n            s.seller_id,\n            g.*\n        from sellers s\n        left join geolocation g on s.seller_zip_code_prefix = g.geolocation_zip_code_prefix\n    \"\"\")\n\n    tables = {\n        'fact_order_items': df_fact_order_items,\n        'fact_order_payments': spark.table('order_payments'),\n        'fact_order_reviews': spark.table('order_reviews'),\n        'dim_customers': df_dim_customers,\n        'dim_products': df_dim_products,\n        'dim_sellers': df_dim_sellers\n    }\n\n    for table_name, df in tables.items():\n        write_to_mysql(df, table_name)\n\n    print(\"Đã hoàn thiện quá trình nhập dữ liệu vào database\")\n\nexcept Exception as e:\n    print(f\"Lỗi: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:25:26.875348Z","iopub.execute_input":"2026-01-25T03:25:26.876059Z","iopub.status.idle":"2026-01-25T03:25:27.848836Z","shell.execute_reply.started":"2026-01-25T03:25:26.876024Z","shell.execute_reply":"2026-01-25T03:25:27.847671Z"}},"outputs":[{"name":"stdout","text":"Lỗi: An error occurred (403) when calling the ListObjectsV2 operation: Forbidden\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"spark.stop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-25T03:25:27.851428Z","iopub.execute_input":"2026-01-25T03:25:27.851836Z","iopub.status.idle":"2026-01-25T03:25:28.046637Z","shell.execute_reply.started":"2026-01-25T03:25:27.851809Z","shell.execute_reply":"2026-01-25T03:25:28.045795Z"}},"outputs":[],"execution_count":7}]}