{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/duysakura/visualization-2-0?scriptVersionId=294152787\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:32:10.480512Z","iopub.execute_input":"2026-01-26T17:32:10.48098Z","iopub.status.idle":"2026-01-26T17:32:14.223866Z","shell.execute_reply.started":"2026-01-26T17:32:10.480945Z","shell.execute_reply":"2026-01-26T17:32:14.222844Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import boto3\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom kaggle_secrets import UserSecretsClient","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:32:14.225843Z","iopub.execute_input":"2026-01-26T17:32:14.226128Z","iopub.status.idle":"2026-01-26T17:32:14.230707Z","shell.execute_reply.started":"2026-01-26T17:32:14.226097Z","shell.execute_reply":"2026-01-26T17:32:14.22989Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nMINIO_ACCESS_KEY = user_secrets.get_secret(\"MINIO_ACCESS_KEY\")\nMINIO_ENDPOINT = user_secrets.get_secret(\"MINIO_ENDPOINT\")\nMINIO_SECRET_KEY = user_secrets.get_secret(\"MINIO_SECRET_KEY\")\nMINIO_BUCKET_NAME = 'olist-data'\nMINIO_CONFIG = {\n    'endpoint_url': MINIO_ENDPOINT,\n    'aws_access_key_id': MINIO_ACCESS_KEY,\n    'aws_secret_access_key': MINIO_SECRET_KEY\n}\n\nMYSQL_URL = user_secrets.get_secret(\"MYSQL_URL\")\nMYSQL_PROPERTIES = {\n    'user': user_secrets.get_secret(\"MYSQL_USER\"),\n    'password': user_secrets.get_secret(\"MYSQL_PASSWORD\"),\n    'driver': user_secrets.get_secret(\"MYSQL_DRIVER\")\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:32:14.231731Z","iopub.execute_input":"2026-01-26T17:32:14.232137Z","iopub.status.idle":"2026-01-26T17:32:14.540941Z","shell.execute_reply.started":"2026-01-26T17:32:14.2321Z","shell.execute_reply":"2026-01-26T17:32:14.540111Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def write_to_mysql(df, table_name):\n    try:\n        jdbc_url = MYSQL_URL\n\n        params = [\n            \"rewriteBatchedStatements=true\",\n            \"allowPublicKeyRetrieval=true\",\n            \"useSSL=false\"\n        ]\n\n        if \"?\" in jdbc_url:\n            jdbc_url = jdbc_url + \"&\" + \"&\".join(params)\n        else:\n            jdbc_url = jdbc_url + \"?\" + \"&\".join(params)\n\n        df.coalesce(4).write.jdbc(\n            url=jdbc_url,\n            table=table_name,\n            mode='overwrite',\n            properties={\n                **MYSQL_PROPERTIES,\n                \"batchsize\": \"5000\",\n                \"isolationLevel\": \"NONE\"\n            }\n        )\n        \n        print(f\"Lưu bảng {table_name} thành công\")\n        \n    except Exception as e:\n        print(f\"Lỗi khi ghi bảng {table_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:32:14.542037Z","iopub.execute_input":"2026-01-26T17:32:14.542371Z","iopub.status.idle":"2026-01-26T17:32:14.549001Z","shell.execute_reply.started":"2026-01-26T17:32:14.542331Z","shell.execute_reply":"2026-01-26T17:32:14.548044Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"packages = [\n    \"org.apache.hadoop:hadoop-aws:3.3.4\",            \n    \"com.amazonaws:aws-java-sdk-bundle:1.12.540\",\n    \"com.mysql:mysql-connector-j:8.2.0\"\n]\n\nspark = SparkSession.builder \\\n    .appName(\"Olist's Brazilian E-Commerce\") \\\n    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n\nspark.sparkContext.setLogLevel(\"WARN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:32:14.550779Z","iopub.execute_input":"2026-01-26T17:32:14.551108Z","iopub.status.idle":"2026-01-26T17:32:16.082942Z","shell.execute_reply.started":"2026-01-26T17:32:14.55108Z","shell.execute_reply":"2026-01-26T17:32:16.081571Z"}},"outputs":[{"name":"stderr","text":"26/01/26 17:32:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"s3 = boto3.client('s3', **MINIO_CONFIG)\n\ntry:\n    response = s3.list_objects_v2(Bucket=MINIO_BUCKET_NAME)\n\n    if 'Contents' not in response:\n        print(\"Bucket rỗng\")\n        sys.exit(0)\n\n    # I. Load dữ liệu thô từ MinIO và chuyển thành bảng\n    base_url = f\"s3a://{MINIO_BUCKET_NAME}\"\n    for obj in response['Contents']:\n        file_name = obj['Key']\n\n        if not file_name.endswith('.csv'):\n            continue\n\n        df = spark.read.csv(f\"{base_url}/{file_name}\", header=True, multiLine=True, inferSchema=True)\n        table_name = file_name.replace('olist_', '').replace('_dataset.csv', '').replace('.csv', '')\n        df.createOrReplaceTempView(table_name)\n\n    # II. Làm sạch dữ liệu\n    # 1. Chuyển tên sản phẩm trong products về tiếng Anh\n    df_clean_products = spark.sql(\"\"\"\n        select \n            p.*,\n            pt.product_category_name_english\n        from products p\n        left join product_category_name_translation pt on p.product_category_name = pt.product_category_name\n    \"\"\")\n    df_clean_products = df_clean_products.drop(\"product_category_name\")\n    df_clean_products.createOrReplaceTempView(\"products\")\n\n    # 2. Chuẩn hóa geolocation với tọa độ là trung bình các tọa độ và tên thành phố theo chuẩn Title Case, không dấu\n    df_clean_geolocation = spark.sql(\"\"\"\n        select \n            geolocation_zip_code_prefix,\n            avg(geolocation_lat) geolocation_lat,\n            avg(geolocation_lng) geolocation_lng,\n            initcap(\n                translate(\n                    first(geolocation_city), \n                    'áàảãạâấầẩẫậăắằẳẵặéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵđÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỊĐ',\n                    'aaaaaaaaaaaaaaaaaeeeeeeeeeeeiiiiioooooooooooooooooouuuuuuuuuuuyyyyydAAAAAAAAAAAAAAAAAEEEEEEEEEEEIIIIIOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYD'\n                )\n            ) geolocation_city,\n            first(geolocation_state) geolocation_state\n        from geolocation\n        group by geolocation_zip_code_prefix\n    \"\"\")\n    df_clean_geolocation.createOrReplaceTempView(\"geolocation\")\n\n    # 3. Loại bỏ các đơn hàng không giao thành công\n    df_clean_orders = spark.sql(\"\"\"\n        select * from orders\n        where order_delivered_customer_date is not null\n    \"\"\")\n    df_clean_orders.createOrReplaceTempView(\"orders\")\n\n    # 4. Loại bỏ những reviews có ngày tháng không hợp lệ và chuẩn hóa kiểu dữ liệu của các cột ngày tháng\n    df_clean_order_reviews = spark.sql(\"\"\"\n        select * from order_reviews\n        where review_creation_date is not null and review_answer_timestamp is not null\n    \"\"\")\n    df_clean_order_reviews = df_clean_order_reviews \\\n        .withColumn(\"review_creation_date\", col(\"review_creation_date\").cast(\"timestamp\")) \\\n        .withColumn(\"review_answer_timestamp\", col(\"review_answer_timestamp\").cast(\"timestamp\")) \\\n        .withColumnRenamed(\"review_answer_timestamp\", \"review_answer_date\")\n    df_clean_order_reviews.createOrReplaceTempView(\"order_reviews\")\n\n    # III. Làm giàu dữ liệu\n    # 1. Tính thời gian giao hàng và kiểm tra đơn hàng có bị giao muộn không\n    df_enrich_orders = spark.sql(\"\"\"\n        select \n            *,\n            datediff(order_delivered_customer_date, order_purchase_timestamp) actual_delivery_days,\n            (order_delivered_customer_date > order_estimated_delivery_date) is_late\n        from orders\n    \"\"\")\n    df_enrich_orders.createOrReplaceTempView(\"orders\")\n\n    # 2. Tính tổng giá trị đơn hàng trong order_items\n    df_enrich_order_items = spark.sql(\"\"\"\n        select \n            *,\n            round((price + freight_value), 2) total_item_value\n        from order_items\n    \"\"\")\n    df_enrich_order_items.createOrReplaceTempView(\"order_items\")\n\n    # 3. Phân loại các thành phố, bang theo khu vực\n    region_data = [\n        ('AC', 'North'), ('AP', 'North'), ('AM', 'North'), ('PA', 'North'), ('RO', 'North'), ('RR', 'North'), ('TO', 'North'),\n        ('AL', 'Northeast'), ('BA', 'Northeast'), ('CE', 'Northeast'), ('MA', 'Northeast'), ('PB', 'Northeast'), ('PE', 'Northeast'), ('PI', 'Northeast'), ('RN', 'Northeast'), ('SE', 'Northeast'),\n        ('DF', 'Central-West'), ('GO', 'Central-West'), ('MT', 'Central-West'), ('MS', 'Central-West'),\n        ('ES', 'Southeast'), ('MG', 'Southeast'), ('RJ', 'Southeast'), ('SP', 'Southeast'),\n        ('PR', 'South'), ('RS', 'South'), ('SC', 'South')\n    ]\n    df_regions = spark.createDataFrame(region_data, [\"geolocation_state\", \"region\"])\n    df_regions.createOrReplaceTempView(\"regions\")\n    df_enrich_geolocation = spark.sql(\"\"\"\n        select\n            g.*,\n            r.region geolocation_region\n        from geolocation g\n        left join regions r on g.geolocation_state = r.geolocation_state\n    \"\"\")\n    df_enrich_geolocation.createOrReplaceTempView(\"geolocation\")\n\n    # IV. Mô hình hóa\n    df_fact_order_items = spark.sql(\"\"\"\n        select \n            o.order_id,\n            o.customer_id,\n            oi.product_id,\n            oi.seller_id,\n            o.order_purchase_timestamp,\n            o.order_delivered_customer_date,\n            oi.price,\n            oi.freight_value,\n            oi.total_item_value,\n            o.actual_delivery_days,\n            o.is_late\n        from orders o\n        join order_items oi on o.order_id = oi.order_id\n    \"\"\")\n\n    df_dim_customers = spark.sql(\"\"\"\n        select \n            c.customer_id,\n            c.customer_unique_id,\n            g.*\n        from customers c\n        left join geolocation g on c.customer_zip_code_prefix = g.geolocation_zip_code_prefix\n    \"\"\")\n\n    df_dim_products = spark.sql(\"\"\"\n        select \n            product_id,\n            product_category_name_english product_category_name,\n            product_photos_qty,\n            product_weight_g,\n            product_length_cm,\n            product_height_cm,\n            product_width_cm\n        from products\n    \"\"\")\n\n    df_dim_sellers = spark.sql(\"\"\"\n        select \n            s.seller_id,\n            g.*\n        from sellers s\n        left join geolocation g on s.seller_zip_code_prefix = g.geolocation_zip_code_prefix\n    \"\"\")\n\n    tables = {\n        'fact_order_items': df_fact_order_items,\n        'fact_order_payments': spark.table('order_payments'),\n        'fact_order_reviews': spark.table('order_reviews'),\n        'dim_customers': df_dim_customers,\n        'dim_products': df_dim_products,\n        'dim_sellers': df_dim_sellers\n    }\n\n    for table_name, df in tables.items():\n        write_to_mysql(df, table_name)\n\n    print(\"Đã hoàn thiện quá trình nhập dữ liệu vào database\")\n\nexcept Exception as e:\n    print(f\"Lỗi: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:32:16.084332Z","iopub.execute_input":"2026-01-26T17:32:16.084872Z","iopub.status.idle":"2026-01-26T17:35:56.223865Z","shell.execute_reply.started":"2026-01-26T17:32:16.084759Z","shell.execute_reply":"2026-01-26T17:35:56.222979Z"}},"outputs":[{"name":"stderr","text":"26/01/26 17:32:17 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Lưu bảng fact_order_items thành công\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Lưu bảng fact_order_payments thành công\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Lưu bảng fact_order_reviews thành công\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Lưu bảng dim_customers thành công\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Lưu bảng dim_products thành công\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Lưu bảng dim_sellers thành công\nĐã hoàn thiện quá trình nhập dữ liệu vào database\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"spark.stop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-26T17:35:56.225223Z","iopub.execute_input":"2026-01-26T17:35:56.225671Z","iopub.status.idle":"2026-01-26T17:35:56.42795Z","shell.execute_reply.started":"2026-01-26T17:35:56.22564Z","shell.execute_reply":"2026-01-26T17:35:56.427062Z"}},"outputs":[],"execution_count":13}]}